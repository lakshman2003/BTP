{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "transaction_rate = 0.005\n",
    "\n",
    "def newWealth_withTC(W,r,pi_pre,pi_new):\n",
    "    # W_new = torch.scalar_tensor(0,requires_grad=True)\n",
    "    # #W_new = 0\n",
    "    # #W = torch.scalar_tensor(W,requires_grad=True)\n",
    "    # r = torch.tensor(r,requires_grad=True)\n",
    "    # for i in range(n+1):\n",
    "    #     W_new += W*pi_pre[i]*(1+r[i])\n",
    "    # W_new = W_new - W_new*transaction_rate*torch.norm(pi_new-pi_pre,p = 1)\n",
    "    # return torch.tensor(W_new,requires_grad=True)\n",
    "    W = torch.tensor(W, requires_grad=True)\n",
    "    r = torch.tensor(r, requires_grad=True)\n",
    "    pi_pre = torch.tensor(pi_pre, requires_grad=True)\n",
    "    pi_new = torch.tensor(pi_new, requires_grad=True)\n",
    "\n",
    "    n = len(r) - 1\n",
    "    W_new = torch.scalar_tensor(0.0, requires_grad=True)\n",
    "\n",
    "    # Accumulate values in a new tensor without in-place operations\n",
    "    for i in range(n + 1):\n",
    "        W_new = W_new + W * pi_pre[i] * (1 + r[i])\n",
    "\n",
    "    tc = transaction_rate * torch.norm(pi_new - pi_pre, p=1)\n",
    "\n",
    "    # Update W_new with the transaction cost\n",
    "    W_new = W_new - W_new * tc\n",
    "\n",
    "    return W_new\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, crra_coefficient):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.crra_coefficient = crra_coefficient\n",
    "\n",
    "    def forward(self, initial_wealth, initial_portfolio, upper_b, lower_b,returns,input_data):\n",
    "        # Calculate the CRRA utility loss\n",
    "        input_size = upper_b.shape[0]\n",
    "        total_loss = 0\n",
    "        #print(lower_b.shape,upper_b.shape)\n",
    "        for _ in range(input_size):\n",
    "            wealth = [initial_wealth]\n",
    "            portfolio = [initial_portfolio]\n",
    "            pi_star_arrays = []\n",
    "            #print(portfolio)\n",
    "            #constraint_loss = 0\n",
    "            for i in range(T):\n",
    "                w = wealth[-1]\n",
    "                pi = portfolio[-1]\n",
    "                pi_new = pi.clone().detach()\n",
    "                pi_new = pi_new[:-1]\n",
    "\n",
    "                closest_p_ind,__ = closest_probability_distn(input_data[_][i])\n",
    "                pi_star = torch.tensor(optimal_pi_star[(closest_p_ind,i)],requires_grad=True)\n",
    "                pi_star_arrays.append(pi_star)\n",
    "                pi_star = pi_star[:-1]\n",
    "                #print(\"Pi star: \",pi_star)\n",
    "                #print(lb,ub)\n",
    "                # print(\"Pi_star :\",pi_star)\n",
    "                u_b = upper_b[_][i]\n",
    "                l_b = lower_b[_][i]\n",
    "\n",
    "                #print(\"Boundaries: \",l_b,u_b)\n",
    "                #constraint_loss+= torch.sum(torch.relu(l_b - u_b))*1000\n",
    "                #print(constraint_loss)\n",
    "                # print(\"Pi: \",pi_new)\n",
    "                # print(\"Upper_delta: \",u_b)\n",
    "                # print(\"Lower delta: \",l_b)\n",
    "                # for j in range(n):\n",
    "                #     pi_new[j] = max(max(-1.0,l_b[j]), min(min(u_b[j],1), pi_new[j]))\n",
    "                u_b = torch.min(torch.ones(n),torch.add(pi_star,u_b))\n",
    "                l_b = torch.max(-torch.ones(n),torch.add(pi_star,-l_b))\n",
    "                pi_new = torch.max(l_b,torch.min(u_b,pi_new))\n",
    "                # print(\"Upper boundary: \",u_b)\n",
    "                # print(\"Lower_boundary: \",l_b)\n",
    "                # print(\"Pi: \",pi_new)\n",
    "                risk_free_allocation = torch.tensor([1-torch.sum(pi_new)],requires_grad=True)\n",
    "                #print(risk_free_allocation)\n",
    "                #print(risk_free_allocation)\n",
    "                pi_new = torch.stack((pi_new,risk_free_allocation)).reshape(n+1)\n",
    "                #print(pi_new)\n",
    "                portfolio.append(torch.tensor(pi_new,requires_grad=True))\n",
    "                #print(portfolio)\n",
    "                new_wealth = newWealth_withTC(W=w,r = returns[_][i], pi_pre= pi,pi_new= pi_new)\n",
    "                #print(new_wealth)\n",
    "                wealth.append(new_wealth)\n",
    "\n",
    "            #print(wealth)\n",
    "            loss = -((wealth[-1]) ** self.crra_coefficient) / self.crra_coefficient \n",
    "\n",
    "            total_loss+= loss\n",
    "            # print(\"Portfolio: \",portfolio)\n",
    "            # print(\"Wealth: \",wealth)\n",
    "            # print(\"Optimal_portfolios: \",pi_star_arrays)\n",
    "        \n",
    "        return total_loss/input_size\n",
    "\n",
    "# Define your RNN cell with two neural networks\n",
    "class CustomRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        #self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size = output_size, num_layers = 1, batch_first=True)\n",
    "        #self.fc1 = nn.Linear(input_size, output_size)\n",
    "          #Network for \"no-trade\"\n",
    "        # self.fc1 = nn.Sequential(\n",
    "        #     nn.Linear(input_size,80,bias = True),\n",
    "        #     nn.Sigmoid(),\n",
    "        #     nn.Linear(80,40),\n",
    "        #     nn.Sigmoid(),\n",
    "        #     nn.Linear(40,20),\n",
    "        #     nn.Sigmoid(),\n",
    "        #     nn.Linear(20,output_size),\n",
    "        #     nn.Sigmoid()\n",
    "        # )\n",
    "        # # # self.fc2 = nn.Linear(input_size, output_size)  # Network for \"x-zone\"\n",
    "        # # self.fc2 = nn.Sequential(\n",
    "        # #     nn.Linear(input_size,80),\n",
    "        # #     nn.Tanh(),\n",
    "        # #     nn.Linear(80,40),\n",
    "        # #     nn.ReLU(),\n",
    "        # #     nn.Linear(40,20),\n",
    "        # #     nn.ReLU(),\n",
    "        # #     nn.Linear(20,output_size),\n",
    "        # #     nn.Tanh()\n",
    "        # # )\n",
    "        # self.fc2 = nn.Sequential(\n",
    "        #     nn.Linear(input_size,80,bias = False),\n",
    "        #     nn.Sigmoid(),\n",
    "        #     nn.Linear(80,40,bias = False),\n",
    "        #     nn.Sigmoid(),\n",
    "        #     nn.Linear(40,20,bias = False),\n",
    "        #     nn.Sigmoid(),\n",
    "        #     nn.Linear(20,output_size,bias = False),\n",
    "        #     nn.Sigmoid()\n",
    "        # )\n",
    "\n",
    "        # self.fc1 = nn.Sequential(\n",
    "        #     nn.Linear(input_size, 80, bias=True),\n",
    "        #     nn.Tanh(),  # ReLU activation for better sensitivity\n",
    "        #     #nn.BatchNorm1d(80),  # Batch normalization\n",
    "        #     nn.Linear(80, 40, bias=True),\n",
    "        #     nn.Tanh(),  # ReLU activation\n",
    "        #     #nn.BatchNorm1d(40),  # Batch normalization\n",
    "        #     nn.Linear(40, 20, bias=True),\n",
    "        #     nn.Tanh(),  # ReLU activation\n",
    "        #     #nn.BatchNorm1d(20),  # Batch normalization\n",
    "        #     nn.Linear(20, output_size),\n",
    "        #     nn.Sigmoid()\n",
    "        # )   \n",
    "\n",
    "        # self.fc2 = nn.Sequential(\n",
    "        #     nn.Linear(input_size, 80, bias=True),\n",
    "        #     nn.Tanh(),  # Tanh activation\n",
    "        #     #nn.BatchNorm1d(80),  # Batch normalization\n",
    "        #     nn.Linear(80, 40, bias=True),\n",
    "        #     nn.Tanh(),  # Tanh activation\n",
    "        #     #nn.BatchNorm1d(40),  # Batch normalization\n",
    "        #     nn.Linear(40, 20, bias=True),\n",
    "        #     nn.Tanh(),  # Tanh activation\n",
    "        #     #nn.BatchNorm1d(20),  # Batch normalization\n",
    "        #     nn.Linear(20, output_size, bias=True),\n",
    "        #     nn.Sigmoid()\n",
    "        # )   \n",
    "\n",
    "        #self.observed_boundaries = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_to_nn= x.reshape(-1,N)\n",
    "        #print(x.shape)\n",
    "        # out_l = self.fc1(x*1000)\n",
    "        # out_u = self.fc2(x*1000)\n",
    "        # print(out_l)\n",
    "        # print(out_u)\n",
    "        #print(out.shape)\n",
    "        # out_no_trade = self.fc1(out)\n",
    "        # out_x_zone = self.fc2(out)\n",
    "        #print(out_u.shape)\n",
    "        # lower_b = out_l.reshape(-1,T,n)\n",
    "        # upper_b = out_u.reshape(-1,T,n)\n",
    "        #print(x.shape)\n",
    "        # lower_b = torch.tanh(self.fc1(x))\n",
    "        # upper_b = torch.tanh(self.fc2(x))\n",
    "        #upper_b = torch.maximum(lower_b,upper_b)\n",
    "        out,_ = self.rnn(x)\n",
    "        out = torch.sigmoid(x)\n",
    "        out_l = out[:,:,:n]\n",
    "        out_u = out[:,:,n:]\n",
    "        return torch.tensor(out_l,requires_grad=True),torch.tensor(out_u,requires_grad=True)\n",
    "\n",
    "# Example usage of the custom loss function and RNN cell\n",
    "input_size = N  # Replace with the appropriate input size\n",
    "#hidden_size = 150  # Replace with the appropriate hidden size\n",
    "output_size = 2*n # 2 output units, \"no-trade\" and \"x-zone\" decisions\n",
    "crra_coefficient = 1  # CRRA coefficient, adjust as needed\n",
    "\n",
    "rnn_cell = CustomRNNCell(input_size, output_size)\n",
    "custom_loss = CustomLoss(crra_coefficient)\n",
    "\n",
    "# Generate some example data\n",
    "#input_data = torch.randn(T, input_size)  # (batch_size, sequence_length, input_size)\n",
    "#target_data = torch.randn(T, output_size)  # (batch_size, sequence_length, output_size)\n",
    "#regime_probs = torch.rand(T,2)  # Probability vector over regimes\n",
    "\n",
    "n_samples = 1000\n",
    "input_data = []\n",
    "return_samples= []\n",
    "cnt0 = 0\n",
    "plot_x = []\n",
    "for i in range(n_samples):\n",
    "    sample = generate_monte_carlo_sample()\n",
    "    if(sample[0][0]==0):\n",
    "        cnt0+=1\n",
    "    returns_for_this_sample = []\n",
    "    for time in range(T):\n",
    "        returns_for_this_sample.append(sample[time][1])\n",
    "    return_samples.append(returns_for_this_sample)\n",
    "    #return_samples.append()\n",
    "    initial_p = np.random.rand(N)\n",
    "    plot_x.append(initial_p[0])\n",
    "    initial_p/=np.sum(initial_p)\n",
    "    input_to_rnn = [initial_p]\n",
    "    for time in range(T-1):\n",
    "        p = input_to_rnn[-1]\n",
    "        p_new = updateBelief_for_RNN(sample[time][1],p)\n",
    "        #p_new = closest_probability_distn(p_new)[1]\n",
    "        plot_x.append(p_new[0])\n",
    "        input_to_rnn.append(p_new)\n",
    "    input_data.append(input_to_rnn)\n",
    "\n",
    "print(cnt0)\n",
    "print(len(plot_x))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(plot_x,bins = 100)\n",
    "plt.show()\n",
    "\n",
    "print(\"Input data: \",input_data[0])\n",
    "print(\"Return_Samples: \",return_samples[0])\n",
    "input_data = torch.tensor(input_data,dtype= torch.float32,requires_grad=True)\n",
    "#return_samples = torch.tensor(return_samples,dtype = torch.float32)\n",
    "\n",
    "optimizer = optim.Adam(rnn_cell.parameters(), lr=0.0001)\n",
    "#print(reverse_index)\n",
    "# Forward pass and compute lossnp.ones(n+1)/(n+1)\n",
    "for iter in range(10):\n",
    "    lb,ub = rnn_cell(input_data)\n",
    "    #print(lb.shape)\n",
    "    #print(lb,ub)\n",
    "    initial_wealth = torch.scalar_tensor(1000,requires_grad=True)\n",
    "    initial_portfolio = torch.rand(n+1,requires_grad=True)\n",
    "    initial_portfolio = initial_portfolio/torch.sum(initial_portfolio)\n",
    "    loss = custom_loss(initial_wealth= initial_wealth,initial_portfolio = initial_portfolio,upper_b = ub,lower_b = lb,returns = np.array(return_samples),input_data = input_data)\n",
    "\n",
    "    print(\"Loss: \",loss)\n",
    "    # You can then use these losses to perform backpropagation and optimize your RNN cell using an optimizer like Adam.\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
