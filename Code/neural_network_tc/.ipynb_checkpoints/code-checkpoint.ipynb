{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming Approach for Single asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.94302828]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, [0.05379901614137653, 0.000308]],\n",
       " [1, [0.004523396286392601, 0.000308]],\n",
       " [1, [0.07994826873175596, 0.000308]],\n",
       " [1, [-0.005543430617731167, 0.000308]],\n",
       " [1, [0.01667166058346663, 0.000308]],\n",
       " [1, [-0.008086039256130297, 0.000308]],\n",
       " [0, [0.007530925821843405, 0.000308]],\n",
       " [0, [0.01640339972200108, 0.000308]],\n",
       " [1, [-0.06813403693738371, 0.000308]],\n",
       " [1, [-0.00020191520172587298, 0.000308]],\n",
       " [0, [0.008394907233274252, 0.000308]],\n",
       " [1, [-0.022944754072389762, 0.000308]],\n",
       " [0, [-0.009287167020347167, 0.000308]],\n",
       " [1, [-0.005190342286427813, 0.000308]],\n",
       " [1, [0.008357533712193443, 0.000308]],\n",
       " [0, [0.0050262419713888095, 0.000308]],\n",
       " [1, [-0.01941058317809193, 0.000308]],\n",
       " [0, [-0.013735099599652778, 0.000308]],\n",
       " [1, [0.004518744616786127, 0.000308]],\n",
       " [1, [-0.0023920651496659047, 0.000308]],\n",
       " [1, [0.05297341504036924, 0.000308]],\n",
       " [0, [-0.004661082182663059, 0.000308]],\n",
       " [1, [0.05856542126706395, 0.000308]],\n",
       " [0, [-0.014215802754221943, 0.000308]],\n",
       " [1, [-0.035975375992100544, 0.000308]],\n",
       " [1, [-0.054369741676326466, 0.000308]],\n",
       " [0, [-0.0015533914038188142, 0.000308]],\n",
       " [1, [0.06499082200751627, 0.000308]],\n",
       " [1, [-0.027097604921166866, 0.000308]],\n",
       " [0, [-0.018933626858997958, 0.000308]],\n",
       " [1, [0.027076565749922447, 0.000308]],\n",
       " [1, [-0.04928735402322056, 0.000308]],\n",
       " [0, [-0.0204523771271245, 0.000308]],\n",
       " [0, [0.0015630126524759746, 0.000308]],\n",
       " [1, [0.03195419945862348, 0.000308]],\n",
       " [0, [-0.0017510576634501601, 0.000308]],\n",
       " [1, [0.0065537898454876495, 0.000308]],\n",
       " [0, [0.01966092168201921, 0.000308]],\n",
       " [0, [-0.004885343789617401, 0.000308]],\n",
       " [0, [-0.0022916055196029136, 0.000308]],\n",
       " [1, [-0.00800988707993217, 0.000308]],\n",
       " [0, [-0.04112379489159329, 0.000308]],\n",
       " [1, [-0.013644943169824496, 0.000308]],\n",
       " [0, [0.02488422223794037, 0.000308]],\n",
       " [1, [0.025176027201019996, 0.000308]],\n",
       " [1, [0.017314118920531776, 0.000308]],\n",
       " [1, [-0.009162965964482489, 0.000308]],\n",
       " [0, [0.014614830738126243, 0.000308]],\n",
       " [0, [0.0028810895244347516, 0.000308]],\n",
       " [0, [0.019819968435547845, 0.000308]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "def generate_normal_distribution(n):\n",
    "\n",
    "    L = np.random.rand(n, n)\n",
    "    cov_matrix = np.dot(L, L.T)\n",
    "\n",
    "    mean = np.random.randn(n)\n",
    "    print(mean)\n",
    "    \n",
    "    return [mean,cov_matrix]\n",
    "\n",
    "#generate_normal_distribution(5)\n",
    "\n",
    "\n",
    "R0 = np.random.randn(1)\n",
    "print(R0)\n",
    "def get_riskfree_price(t):\n",
    "    return R0*np.exp(rf*t)\n",
    "\n",
    "n = 1\n",
    "regimes = []\n",
    "N = 2\n",
    "T= 50\n",
    "rf = 0.0154/50\n",
    "gamma = -1\n",
    "tpm = np.array([[0.981,0.019],[0.047,0.953]])\n",
    "#transaction_rate = 0.1\n",
    "# for i in range(N):\n",
    "#     regimes.append(generate_normal_distribution(n))\n",
    "\n",
    "regimes = [[[0.00312],[[0.00022]]],[[-0.00175],[[0.00116]]]]\n",
    "\n",
    "def generate_scenario(prob,t):\n",
    "    # curren\n",
    "    # returns_for_regimes = []\n",
    "    # for i in range(N):\n",
    "    #     sample = np.random.multivariate_normal(mean = regimes[i][0], cov= regimes[i][1])\n",
    "    #     returns_for_regimes.append(sample)\n",
    "\n",
    "    # returns_for_regimes = np.array(returns_for_regimes)\n",
    "\n",
    "    # sample = []\n",
    "    # for i in range(n):\n",
    "    #     value = 0\n",
    "    #     for j in range(N):\n",
    "    #         value+= prob[j]*returns_for_regimes[j][i]\n",
    "    #     sample.append(value)\n",
    "    # sample.append(get_riskfree_price(t)[0])\n",
    "    current_regime = np.random.choice(N,p = prob)\n",
    "    #print(current_regime)\n",
    "    sample = list(np.random.multivariate_normal(mean = regimes[current_regime][0], cov= regimes[current_regime][1]))\n",
    "    #sample.append(get_riskfree_price(t)[0])\n",
    "    sample.append(rf)\n",
    "    return np.array(sample)\n",
    "\n",
    "def generate_monte_carlo_sample():\n",
    "    sample = []\n",
    "    curr_prob = np.random.rand(N)\n",
    "    curr_prob/=np.sum(curr_prob)\n",
    "    curr_regime = np.random.choice(range(N), p = curr_prob)\n",
    "    #print(regimes[curr_regime][1])\n",
    "    returns = list(np.random.multivariate_normal(mean = regimes[curr_regime][0], cov= regimes[curr_regime][1]))\n",
    "    returns.append(rf)\n",
    "    sample.append([curr_regime,returns])\n",
    "    for i in range(T-1):\n",
    "        curr_prob = tpm @ curr_prob\n",
    "        curr_prob/=np.sum(curr_prob)\n",
    "        curr_regime = np.random.choice(range(N), p = curr_prob)\n",
    "        returns = list(np.random.multivariate_normal(mean = regimes[curr_regime][0], cov= regimes[curr_regime][1]))\n",
    "        returns.append(rf)\n",
    "        sample.append([curr_regime,returns])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "generate_monte_carlo_sample()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#generate_scenario([0.1,0.1,0.3,0.2,0.3],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00175], [[0.00116]]]\n"
     ]
    }
   ],
   "source": [
    "print(regimes[1][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilty set initialization over different regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from scipy.optimize import minimize\n",
    "discrete_prob = 50\n",
    "prob_set = []\n",
    "for i in range(discrete_prob+1):\n",
    "    prob_set.append(1/(discrete_prob)*i)\n",
    "\n",
    "combinations = product(prob_set, repeat=N)\n",
    "possible_probabities = [vector for vector in combinations if sum(vector) == 1]\n",
    "reverse_index = {}\n",
    "for _,p in enumerate(possible_probabities):\n",
    "    reverse_index[p]= _\n",
    "\n",
    "possible_probabities = [np.array(x) for x in possible_probabities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     curr_pi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mdelta_pi\n\u001b[0;32m      8\u001b[0m pi_combinations \u001b[38;5;241m=\u001b[39m product(pi_set,repeat\u001b[38;5;241m=\u001b[39mn)\n\u001b[1;32m----> 9\u001b[0m possible_pi \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(vector)\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28msum\u001b[39m(vector)) \u001b[38;5;28;01mfor\u001b[39;00m vector \u001b[38;5;129;01min\u001b[39;00m pi_combinations]\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m     curr_pi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mdelta_pi\n\u001b[0;32m      8\u001b[0m pi_combinations \u001b[38;5;241m=\u001b[39m product(pi_set,repeat\u001b[38;5;241m=\u001b[39mn)\n\u001b[1;32m----> 9\u001b[0m possible_pi \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28msum\u001b[39m(vector)) \u001b[38;5;28;01mfor\u001b[39;00m vector \u001b[38;5;129;01min\u001b[39;00m pi_combinations]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "delta_pi = 0.01\n",
    "pi_set = []\n",
    "curr_pi = -1\n",
    "while curr_pi<=1:\n",
    "    pi_set.append(curr_pi)\n",
    "    curr_pi+=delta_pi\n",
    "\n",
    "pi_combinations = product(pi_set,repeat=n)\n",
    "possible_pi = [np.array(vector for vector in pi_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<itertools.product object at 0x0000017AC9168D80>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_probability_distn(p_in):\n",
    "    lowest_norm = float('inf')  # Set to positive infinity initially\n",
    "    ind = 0\n",
    "    #p_in = p_in.detach().numpy()\n",
    "    # Iterate through the set and calculate the norm for each element\n",
    "    for _,array in enumerate(possible_probabities):\n",
    "        norm = np.linalg.norm(p_in - array)  # Calculate the Euclidean norm\n",
    "        if norm < lowest_norm:\n",
    "            lowest_norm = norm\n",
    "            closest_array = array\n",
    "            ind = _\n",
    "\n",
    "    return ind,closest_array\n",
    "\n",
    "def updateBelief(r,p):\n",
    "    #print(r,p)\n",
    "    p_new = []\n",
    "    #print(r)\n",
    "    r = r[:-1]\n",
    "    density = np.zeros(N);\n",
    "    for i in range(N):\n",
    "        mvn = multivariate_normal(mean=regimes[i][0], cov = regimes[i][1])\n",
    "        density[i]= mvn.pdf(r)\n",
    "\n",
    "    p_new = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            p_new[i]+= density[j]*tpm[j][i]*p[j]\n",
    "\n",
    "    p_new/=np.sum(p_new)\n",
    "    closest_p_ind,_ = closest_probability_distn(p_new)\n",
    "    return p_new,closest_p_ind\n",
    "\n",
    "def newWealth(r, pi):\n",
    "    W_new = 0\n",
    "    for i in range(n+1):\n",
    "        W_new+= pi[i]*(1+r[i])\n",
    "    return W_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully\n",
      "0 49 [0. 1.] [-1.  2.]\n",
      "Optimization terminated successfully\n",
      "0 49 [0.02 0.98] [-1.  2.]\n",
      "Optimization terminated successfully\n",
      "0 49 [0.04 0.96] [0.5 0.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m initial_pi_guess\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(initial_pi_guess)\u001b[38;5;66;03m# Starting with equal weights\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Minimize the negative of the objective function to maximize\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m pi_star \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43minitial_pi_guess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpi_u\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(pi_star\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m     45\u001b[0m pi_star \u001b[38;5;241m=\u001b[39m pi_star\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:631\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_cobyla(fun, x0, args, constraints, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslsqp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m    632\u001b[0m                            constraints, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    635\u001b[0m                                         bounds, constraints,\n\u001b[0;32m    636\u001b[0m                                         callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\slsqp.py:437\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    434\u001b[0m     c \u001b[38;5;241m=\u001b[39m _eval_constraint(x, cons)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# gradient evaluation required\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m     g \u001b[38;5;241m=\u001b[39m append(\u001b[43mwrapped_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m    438\u001b[0m     a \u001b[38;5;241m=\u001b[39m _eval_con_normals(x, cons, la, n, m, meq, mieq)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m majiter \u001b[38;5;241m>\u001b[39m majiter_prev:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# call callback if major iteration has incremented\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:275\u001b[0m, in \u001b[0;36m_clip_x_for_func.<locals>.eval\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(x):\n\u001b[0;32m    274\u001b[0m     x \u001b[38;5;241m=\u001b[39m _check_clip_x(x, bounds)\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:255\u001b[0m, in \u001b[0;36mScalarFunction.grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 255\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:238\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m--> 238\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m approx_derivative(fun_wrapped, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, f0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf,\n\u001b[0;32m    156\u001b[0m                            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfinite_diff_options)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:486\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:557\u001b[0m, in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    555\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[0;32m    556\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[0;32m    559\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:437\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[1;32m--> 437\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(fun(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:134\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     36\u001b[0m initial_pi_guess\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(initial_pi_guess)\u001b[38;5;66;03m# Starting with equal weights\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Minimize the negative of the objective function to maximize\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m pi_star \u001b[38;5;241m=\u001b[39m minimize(\u001b[38;5;28;01mlambda\u001b[39;00m x: (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,  initial_pi_guess,\n\u001b[0;32m     40\u001b[0m                    constraints\u001b[38;5;241m=\u001b[39mconstraints,\n\u001b[0;32m     41\u001b[0m                     bounds \u001b[38;5;241m=\u001b[39m [(pi_l,pi_u) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)] \u001b[38;5;241m+\u001b[39m [(\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m)]\n\u001b[0;32m     42\u001b[0m                     )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(pi_star\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m     45\u001b[0m pi_star \u001b[38;5;241m=\u001b[39m pi_star\u001b[38;5;241m.\u001b[39mx\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mobjective_function\u001b[1;34m(pi)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_function\u001b[39m(pi):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39maverage([(newWealth(returns_at_this_time[i],pi)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgamma)\u001b[38;5;241m*\u001b[39mV[(updateBelief(returns_at_this_time[i],possible_probabities[_])[\u001b[38;5;241m1\u001b[39m],t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(M)])\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_function\u001b[39m(pi):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39maverage([(newWealth(returns_at_this_time[i],pi)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgamma)\u001b[38;5;241m*\u001b[39mV[(\u001b[43mupdateBelief\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturns_at_this_time\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpossible_probabities\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m],t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(M)])\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mupdateBelief\u001b[1;34m(r, p)\u001b[0m\n\u001b[0;32m     28\u001b[0m         p_new[i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m density[j]\u001b[38;5;241m*\u001b[39mtpm[j][i]\u001b[38;5;241m*\u001b[39mp[j]\n\u001b[0;32m     30\u001b[0m p_new\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(p_new)\n\u001b[1;32m---> 31\u001b[0m closest_p_ind,_ \u001b[38;5;241m=\u001b[39m \u001b[43mclosest_probability_distn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p_new,closest_p_ind\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mclosest_probability_distn\u001b[1;34m(p_in)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#p_in = p_in.detach().numpy()\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate through the set and calculate the norm for each element\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _,array \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(possible_probabities):\n\u001b[1;32m----> 7\u001b[0m     norm \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculate the Euclidean norm\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;241m<\u001b[39m lowest_norm:\n\u001b[0;32m      9\u001b[0m         lowest_norm \u001b[38;5;241m=\u001b[39m norm\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:2516\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2514\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m dot(x\u001b[38;5;241m.\u001b[39mreal, x\u001b[38;5;241m.\u001b[39mreal) \u001b[38;5;241m+\u001b[39m dot(x\u001b[38;5;241m.\u001b[39mimag, x\u001b[38;5;241m.\u001b[39mimag)\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2516\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2517\u001b[0m ret \u001b[38;5;241m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "V = {}\n",
    "for _ in range(len(possible_probabities)):\n",
    "    V[(_,T)] = (1**gamma)/gamma\n",
    "\n",
    "t = T-1\n",
    "M = 1000\n",
    "pi_l = -1.0\n",
    "pi_u = 1.0\n",
    "\n",
    "   \n",
    "optimal_pi_star = {}\n",
    "\n",
    "# scenarios = []_star = {}\n",
    "for iter in range(1):\n",
    "    # scenarios = []\n",
    "    # for j in range(M):\n",
    "    #     sample = generate_monte_carlo_sample()\n",
    "    #     scenarios.append(sample)\n",
    "    #print(scenarios)\n",
    "    t = T-1\n",
    "    while (t>=0):\n",
    "        #returns_at_this_time = [scenarios[i][t][1] for i in range(M)]\n",
    "        #print(returns_at_this_time)\n",
    "        for _ in range(len(possible_probabities)):\n",
    "            returns_at_this_time = []\n",
    "            for j in range(M):\n",
    "                sample = generate_scenario(possible_probabities[_],t)\n",
    "                returns_at_this_time.append(sample)\n",
    "            def objective_function(pi):\n",
    "                return np.average([(newWealth(returns_at_this_time[i],pi)**gamma)*V[(updateBelief(returns_at_this_time[i],possible_probabities[_])[1],t+1)] for i in range(M)])\n",
    "\n",
    "            constraints = ([{'type': 'eq', 'fun': lambda pi: np.sum(pi) - 1}])  # Example constraint: pi sums to 1\n",
    "\n",
    "            # Initial guess for pi (dimension n)\n",
    "            initial_pi_guess = np.ones(n+1) \n",
    "            initial_pi_guess/=np.sum(initial_pi_guess)# Starting with equal weights\n",
    "\n",
    "            # Minimize the negative of the objective function to maximize\n",
    "            pi_star = minimize(lambda x: (-1)*objective_function(x),  initial_pi_guess,\n",
    "                               constraints=constraints,\n",
    "                                bounds = [(pi_l,pi_u) for i in range(n)] + [(None,None)]\n",
    "                                )\n",
    "\n",
    "            print(pi_star.message)\n",
    "            pi_star = pi_star.x\n",
    "            # for i in range(n+1):\n",
    "            #     pi_star[i] = max(pi_l,min(pi_u,pi_star[i]))\n",
    "            V[(_,t)] = np.average([(newWealth(returns_at_this_time[i],pi_star)**gamma)*V[(updateBelief(returns_at_this_time[i],possible_probabities[_])[1],t+1)] for i in range(M)])\n",
    "            optimal_pi_star[(_,t)] = pi_star\n",
    "            print(iter,t,possible_probabities[_],pi_star)\n",
    "\n",
    "        t= t-1\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xpoints = [x[0] for x in possible_probabities]\n",
    "ypoints = [optimal_pi_star[_,2][0] for _ in range(len(possible_probabities))]\n",
    "\n",
    "plt.plot(xpoints,ypoints)\n",
    "plt.show()\n",
    "\n",
    "print([optimal_pi_star[_,0] for _ in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(len(possible_probabities)):\n",
    "    print(possible_probabities[_])\n",
    "    print(V[(_,0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network using Dynamic Program solution as a startpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# transaction_rate = 0.005\n",
    "\n",
    "# def newWealth_withTC(W,r,pi_pre,pi_new):\n",
    "#     # W_new = torch.scalar_tensor(0,requires_grad=True)\n",
    "#     # #W_new = 0\n",
    "#     # #W = torch.scalar_tensor(W,requires_grad=True)\n",
    "#     # r = torch.tensor(r,requires_grad=True)\n",
    "#     # for i in range(n+1):\n",
    "#     #     W_new += W*pi_pre[i]*(1+r[i])\n",
    "#     # W_new = W_new - W_new*transaction_rate*torch.norm(pi_new-pi_pre,p = 1)\n",
    "#     # return torch.tensor(W_new,requires_grad=True)\n",
    "#     W = torch.tensor(W, requires_grad=True)\n",
    "#     r = torch.tensor(r, requires_grad=True)\n",
    "#     pi_pre = torch.tensor(pi_pre, requires_grad=True)\n",
    "#     pi_new = torch.tensor(pi_new, requires_grad=True)\n",
    "\n",
    "#     n = len(r) - 1\n",
    "#     W_new = torch.scalar_tensor(0.0, requires_grad=True)\n",
    "\n",
    "#     # Accumulate values in a new tensor without in-place operations\n",
    "#     for i in range(n + 1):\n",
    "#         W_new = W_new + W * pi_pre[i] * (1 + r[i])\n",
    "\n",
    "#     tc = transaction_rate * torch.norm(pi_new - pi_pre, p=1)\n",
    "\n",
    "#     # Update W_new with the transaction cost\n",
    "#     W_new = W_new - W_new * tc\n",
    "\n",
    "#     return W_new\n",
    "\n",
    "\n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self, crra_coefficient):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "#         self.crra_coefficient = crra_coefficient\n",
    "\n",
    "#     def forward(self, initial_wealth, initial_portfolio, upper_b, lower_b,returns,input_data):\n",
    "#         # Calculate the CRRA utility loss\n",
    "#         input_size = upper_b.shape[0]\n",
    "#         total_loss = 0\n",
    "#         #print(lower_b.shape,upper_b.shape)\n",
    "#         for _ in range(input_size):\n",
    "#             wealth = [initial_wealth]\n",
    "#             portfolio = [initial_portfolio]\n",
    "#             pi_star_arrays = []\n",
    "#             #print(portfolio)\n",
    "#             #constraint_loss = 0\n",
    "#             for i in range(T):\n",
    "#                 w = wealth[-1]\n",
    "#                 pi = portfolio[-1]\n",
    "#                 pi_new = pi.clone().detach()\n",
    "#                 pi_new = pi_new[:-1]\n",
    "\n",
    "#                 closest_p_ind,__ = closest_probability_distn(input_data[_][i])\n",
    "#                 pi_star = torch.tensor(optimal_pi_star[(closest_p_ind,i)],requires_grad=True)\n",
    "#                 pi_star_arrays.append(pi_star)\n",
    "#                 pi_star = pi_star[:-1]\n",
    "#                 #print(\"Pi star: \",pi_star)\n",
    "#                 #print(lb,ub)\n",
    "#                 # print(\"Pi_star :\",pi_star)\n",
    "#                 u_b = upper_b[_][i]\n",
    "#                 l_b = lower_b[_][i]\n",
    "\n",
    "#                 #print(\"Boundaries: \",l_b,u_b)\n",
    "#                 #constraint_loss+= torch.sum(torch.relu(l_b - u_b))*1000\n",
    "#                 #print(constraint_loss)\n",
    "#                 # print(\"Pi: \",pi_new)\n",
    "#                 # print(\"Upper_delta: \",u_b)\n",
    "#                 # print(\"Lower delta: \",l_b)\n",
    "#                 # for j in range(n):\n",
    "#                 #     pi_new[j] = max(max(-1.0,l_b[j]), min(min(u_b[j],1), pi_new[j]))\n",
    "#                 u_b = torch.min(torch.ones(n),torch.add(pi_star,u_b))\n",
    "#                 l_b = torch.max(-torch.ones(n),torch.add(pi_star,-l_b))\n",
    "#                 pi_new = torch.max(l_b,torch.min(u_b,pi_new))\n",
    "#                 # print(\"Upper boundary: \",u_b)\n",
    "#                 # print(\"Lower_boundary: \",l_b)\n",
    "#                 # print(\"Pi: \",pi_new)\n",
    "#                 risk_free_allocation = torch.tensor([1-torch.sum(pi_new)],requires_grad=True)\n",
    "#                 #print(risk_free_allocation)\n",
    "#                 #print(risk_free_allocation)\n",
    "#                 pi_new = torch.stack((pi_new,risk_free_allocation)).reshape(n+1)\n",
    "#                 #print(pi_new)\n",
    "#                 portfolio.append(torch.tensor(pi_new,requires_grad=True))\n",
    "#                 #print(portfolio)\n",
    "#                 new_wealth = newWealth_withTC(W=w,r = returns[_][i], pi_pre= pi,pi_new= pi_new)\n",
    "#                 #print(new_wealth)\n",
    "#                 wealth.append(new_wealth)\n",
    "\n",
    "#             #print(wealth)\n",
    "#             loss = -((wealth[-1]) ** self.crra_coefficient) / self.crra_coefficient \n",
    "\n",
    "#             total_loss+= loss\n",
    "#             # print(\"Portfolio: \",portfolio)\n",
    "#             # print(\"Wealth: \",wealth)\n",
    "#             # print(\"Optimal_portfolios: \",pi_star_arrays)\n",
    "        \n",
    "#         return total_loss/input_size\n",
    "\n",
    "# # Define your RNN cell with two neural networks\n",
    "# class CustomRNNCell(nn.Module):\n",
    "#     def __init__(self, input_size, output_size):\n",
    "#         super(CustomRNNCell, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         #self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.rnn = nn.RNN(input_size, hidden_size = output_size, num_layers = 1, batch_first=True)\n",
    "#         #self.fc1 = nn.Linear(input_size, output_size)\n",
    "#           #Network for \"no-trade\"\n",
    "#         # self.fc1 = nn.Sequential(\n",
    "#         #     nn.Linear(input_size,80,bias = True),\n",
    "#         #     nn.Sigmoid(),\n",
    "#         #     nn.Linear(80,40),\n",
    "#         #     nn.Sigmoid(),\n",
    "#         #     nn.Linear(40,20),\n",
    "#         #     nn.Sigmoid(),\n",
    "#         #     nn.Linear(20,output_size),\n",
    "#         #     nn.Sigmoid()\n",
    "#         # )\n",
    "#         # # # self.fc2 = nn.Linear(input_size, output_size)  # Network for \"x-zone\"\n",
    "#         # # self.fc2 = nn.Sequential(\n",
    "#         # #     nn.Linear(input_size,80),\n",
    "#         # #     nn.Tanh(),\n",
    "#         # #     nn.Linear(80,40),\n",
    "#         # #     nn.ReLU(),\n",
    "#         # #     nn.Linear(40,20),\n",
    "#         # #     nn.ReLU(),\n",
    "#         # #     nn.Linear(20,output_size),\n",
    "#         # #     nn.Tanh()\n",
    "#         # # )\n",
    "#         # self.fc2 = nn.Sequential(\n",
    "#         #     nn.Linear(input_size,80,bias = False),\n",
    "#         #     nn.Sigmoid(),\n",
    "#         #     nn.Linear(80,40,bias = False),\n",
    "#         #     nn.Sigmoid(),\n",
    "#         #     nn.Linear(40,20,bias = False),\n",
    "#         #     nn.Sigmoid(),\n",
    "#         #     nn.Linear(20,output_size,bias = False),\n",
    "#         #     nn.Sigmoid()\n",
    "#         # )\n",
    "\n",
    "#         # self.fc1 = nn.Sequential(\n",
    "#         #     nn.Linear(input_size, 80, bias=True),\n",
    "#         #     nn.Tanh(),  # ReLU activation for better sensitivity\n",
    "#         #     #nn.BatchNorm1d(80),  # Batch normalization\n",
    "#         #     nn.Linear(80, 40, bias=True),\n",
    "#         #     nn.Tanh(),  # ReLU activation\n",
    "#         #     #nn.BatchNorm1d(40),  # Batch normalization\n",
    "#         #     nn.Linear(40, 20, bias=True),\n",
    "#         #     nn.Tanh(),  # ReLU activation\n",
    "#         #     #nn.BatchNorm1d(20),  # Batch normalization\n",
    "#         #     nn.Linear(20, output_size),\n",
    "#         #     nn.Sigmoid()\n",
    "#         # )   \n",
    "\n",
    "#         # self.fc2 = nn.Sequential(\n",
    "#         #     nn.Linear(input_size, 80, bias=True),\n",
    "#         #     nn.Tanh(),  # Tanh activation\n",
    "#         #     #nn.BatchNorm1d(80),  # Batch normalization\n",
    "#         #     nn.Linear(80, 40, bias=True),\n",
    "#         #     nn.Tanh(),  # Tanh activation\n",
    "#         #     #nn.BatchNorm1d(40),  # Batch normalization\n",
    "#         #     nn.Linear(40, 20, bias=True),\n",
    "#         #     nn.Tanh(),  # Tanh activation\n",
    "#         #     #nn.BatchNorm1d(20),  # Batch normalization\n",
    "#         #     nn.Linear(20, output_size, bias=True),\n",
    "#         #     nn.Sigmoid()\n",
    "#         # )   \n",
    "\n",
    "#         #self.observed_boundaries = []\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         input_to_nn= x.reshape(-1,N)\n",
    "#         #print(x.shape)\n",
    "#         # out_l = self.fc1(x*1000)\n",
    "#         # out_u = self.fc2(x*1000)\n",
    "#         # print(out_l)\n",
    "#         # print(out_u)\n",
    "#         #print(out.shape)\n",
    "#         # out_no_trade = self.fc1(out)\n",
    "#         # out_x_zone = self.fc2(out)\n",
    "#         #print(out_u.shape)\n",
    "#         # lower_b = out_l.reshape(-1,T,n)\n",
    "#         # upper_b = out_u.reshape(-1,T,n)\n",
    "#         #print(x.shape)\n",
    "#         # lower_b = torch.tanh(self.fc1(x))\n",
    "#         # upper_b = torch.tanh(self.fc2(x))\n",
    "#         #upper_b = torch.maximum(lower_b,upper_b)\n",
    "#         out,_ = self.rnn(x)\n",
    "#         out = torch.sigmoid(x)\n",
    "#         out_l = out[:,:,:n]\n",
    "#         out_u = out[:,:,n:]\n",
    "#         return torch.tensor(out_l,requires_grad=True),torch.tensor(out_u,requires_grad=True)\n",
    "\n",
    "# # Example usage of the custom loss function and RNN cell\n",
    "# input_size = N  # Replace with the appropriate input size\n",
    "# #hidden_size = 150  # Replace with the appropriate hidden size\n",
    "# output_size = 2*n # 2 output units, \"no-trade\" and \"x-zone\" decisions\n",
    "# crra_coefficient = 1  # CRRA coefficient, adjust as needed\n",
    "\n",
    "# rnn_cell = CustomRNNCell(input_size, output_size)\n",
    "# custom_loss = CustomLoss(crra_coefficient)\n",
    "\n",
    "# # Generate some example data\n",
    "# #input_data = torch.randn(T, input_size)  # (batch_size, sequence_length, input_size)\n",
    "# #target_data = torch.randn(T, output_size)  # (batch_size, sequence_length, output_size)\n",
    "# #regime_probs = torch.rand(T,2)  # Probability vector over regimes\n",
    "\n",
    "# n_samples = 1000\n",
    "# input_data = []\n",
    "# return_samples= []\n",
    "# cnt0 = 0\n",
    "# plot_x = []\n",
    "# for i in range(n_samples):\n",
    "#     sample = generate_monte_carlo_sample()\n",
    "#     if(sample[0][0]==0):\n",
    "#         cnt0+=1\n",
    "#     returns_for_this_sample = []\n",
    "#     for time in range(T):\n",
    "#         returns_for_this_sample.append(sample[time][1])\n",
    "#     return_samples.append(returns_for_this_sample)\n",
    "#     #return_samples.append()\n",
    "#     initial_p = np.random.rand(N)\n",
    "#     plot_x.append(initial_p[0])\n",
    "#     initial_p/=np.sum(initial_p)\n",
    "#     input_to_rnn = [initial_p]\n",
    "#     for time in range(T-1):\n",
    "#         p = input_to_rnn[-1]\n",
    "#         p_new = updateBelief_for_RNN(sample[time][1],p)\n",
    "#         #p_new = closest_probability_distn(p_new)[1]\n",
    "#         plot_x.append(p_new[0])\n",
    "#         input_to_rnn.append(p_new)\n",
    "#     input_data.append(input_to_rnn)\n",
    "\n",
    "# print(cnt0)\n",
    "# print(len(plot_x))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(plot_x,bins = 100)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Input data: \",input_data[0])\n",
    "# print(\"Return_Samples: \",return_samples[0])\n",
    "# input_data = torch.tensor(input_data,dtype= torch.float32,requires_grad=True)\n",
    "# #return_samples = torch.tensor(return_samples,dtype = torch.float32)\n",
    "\n",
    "# optimizer = optim.Adam(rnn_cell.parameters(), lr=0.0001)\n",
    "# #print(reverse_index)\n",
    "# # Forward pass and compute lossnp.ones(n+1)/(n+1)\n",
    "# for iter in range(10):\n",
    "#     lb,ub = rnn_cell(input_data)\n",
    "#     #print(lb.shape)\n",
    "#     #print(lb,ub)\n",
    "#     initial_wealth = torch.scalar_tensor(1000,requires_grad=True)\n",
    "#     initial_portfolio = torch.rand(n+1,requires_grad=True)\n",
    "#     initial_portfolio = initial_portfolio/torch.sum(initial_portfolio)\n",
    "#     loss = custom_loss(initial_wealth= initial_wealth,initial_portfolio = initial_portfolio,upper_b = ub,lower_b = lb,returns = np.array(return_samples),input_data = input_data)\n",
    "\n",
    "#     print(\"Loss: \",loss)\n",
    "#     # You can then use these losses to perform backpropagation and optimize your RNN cell using an optimizer like Adam.\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# x_values = []\n",
    "# upper_b = []\n",
    "# lower_b = []\n",
    "# portfolio_values = []\n",
    "# for p in possible_probabities:\n",
    "#     p_normal = p[0]\n",
    "#     x_values.append(p_normal)\n",
    "#     pi_star = optimal_pi_star[(reverse_index[tuple(p)],1)]\n",
    "#     portfolio_values.append(pi_star[0])\n",
    "#     l_b,u_b = rnn_cell(torch.tensor([[p]*T],dtype = torch.float32))\n",
    "#     lower_b.append(max(-1.0,(pi_star[0]-l_b[0][1][0]).detach().numpy()))\n",
    "#     upper_b.append(min(1.0,(pi_star[0]+u_b[0][1][0]).detach().numpy()))\n",
    "# plt.plot(x_values,lower_b,color= 'g')\n",
    "# plt.plot(x_values,upper_b,color = 'b')\n",
    "# plt.plot(x_values,portfolio_values,color = 'r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# # Define your parameters\n",
    "# import tensorflow as tf\n",
    "\n",
    "def closest_probability_distn_TF(p_in):\n",
    "    lowest_norm = float('inf')  # Set to positive infinity initially\n",
    "    ind = 0\n",
    "    p_in = p_in.numpy()  # Convert to NumPy array\n",
    "    p_in_tf = tf.convert_to_tensor(p_in, dtype=tf.float32)  # Convert back to TensorFlow tensor\n",
    "\n",
    "    # Iterate through the set and calculate the norm for each element\n",
    "    for i, array in enumerate(possible_probabities):\n",
    "        array_tf = tf.convert_to_tensor(array, dtype=tf.float32)\n",
    "        norm = tf.norm(p_in_tf - array_tf, ord='euclidean')  # Calculate the Euclidean norm\n",
    "        if norm < lowest_norm:\n",
    "            lowest_norm = norm\n",
    "            closest_array = array\n",
    "            ind = i\n",
    "\n",
    "    return ind, closest_array\n",
    "\n",
    "# transaction_rate = 0.005\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def newWealth_withTC(W, r, pi_pre, pi_new):\n",
    "#     W = tf.constant(W, dtype=tf.float32)\n",
    "#     r = tf.constant(r, dtype=tf.float32)\n",
    "#     pi_pre = tf.constant(pi_pre, dtype=tf.float32)\n",
    "#     pi_new = tf.constant(pi_new, dtype=tf.float32)\n",
    "\n",
    "#     n = tf.shape(r)[0] - 1\n",
    "#     W_new = tf.Variable(0.0, dtype=tf.float32)\n",
    "#     #print(W,r,pi_pre,pi_new)\n",
    "#     # Accumulate values without in-place operations\n",
    "#     for i in tf.range(n + 1):\n",
    "#         W_new.assign_add(W * pi_pre[i] * (1 + r[i]))\n",
    "\n",
    "#     tc = transaction_rate * tf.norm(pi_new - pi_pre, ord=1)\n",
    "\n",
    "#     # Update W_new with the transaction cost\n",
    "#     W_new.assign_sub(W_new*tc)\n",
    "\n",
    "#     return W_new\n",
    "\n",
    "# # Define your custom loss function\n",
    "\n",
    "# class CustomLossTF(tf.keras.losses.Loss):\n",
    "#     def __init__(self, crra_coefficient):\n",
    "#         super(CustomLossTF, self).__init__()\n",
    "#         self.crra_coefficient = crra_coefficient\n",
    "\n",
    "#     def __call__(self, initial_w, initial_p, upper_b, lower_b, returns, input_data):\n",
    "#         input_size = upper_b.shape[0]\n",
    "#         total_loss = 0.0\n",
    "#         print(input_size)\n",
    "#         for _ in range(input_size):\n",
    "#             print(_)\n",
    "#             wealth = [tf.constant(initial_w,dtype = tf.float32)]\n",
    "#             portfolio = [tf.constant(initial_p,dtype = tf.float32)]\n",
    "\n",
    "#             for i in range(T):\n",
    "#                 w = wealth[-1]\n",
    "#                 pi = portfolio[-1]\n",
    "#                 pi_new = tf.identity(pi)[:-1]\n",
    "\n",
    "#                 # Your logic for updating pi_new and calculating wealth goes here\n",
    "#                 closest_p_ind, __ = closest_probability_distn_TF(input_data[_, i])\n",
    "#                 pi_star = tf.convert_to_tensor(optimal_pi_star[(closest_p_ind, i)],dtype = tf.float32)\n",
    "#                 #pi_star_arrays.append(pi_star)\n",
    "#                 #print(pi_star)\n",
    "#                 pi_star = pi_star[:-1]\n",
    "\n",
    "#                 u_b = upper_b[_, i]\n",
    "#                 l_b = lower_b[_, i]\n",
    "\n",
    "#                 u_b = tf.minimum(tf.ones(n, dtype=tf.float32), pi_star + u_b)\n",
    "#                 l_b = tf.maximum(-tf.ones(n, dtype=tf.float32), pi_star - l_b)\n",
    "#                 #print(l_b,u_b)\n",
    "#                 # Your logic for updating u_b, l_b, and pi_new goes here\n",
    "#                 pi_new = tf.maximum(l_b, tf.minimum(u_b, pi_new))\n",
    "#                 risk_free_allocation = 1 - tf.reduce_sum(pi_new)\n",
    "#                 pi_new = tf.concat([pi_new, [risk_free_allocation]], axis=0)\n",
    "\n",
    "#                 portfolio.append(tf.Variable(pi_new, dtype=tf.float32))\n",
    "#                 new_wealth = tf.convert_to_tensor(newWealth_withTC(W=w, r=returns[_, i], pi_pre=pi, pi_new=pi_new),dtype= tf.float32)\n",
    "#                 #print(new_wealth)\n",
    "#                 wealth.append(new_wealth)\n",
    "\n",
    "\n",
    "#             loss = -tf.math.pow(wealth[-1], self.crra_coefficient) / self.crra_coefficient\n",
    "#             total_loss += loss\n",
    "#             print(wealth,portfolio)\n",
    "\n",
    "#         return total_loss / input_size\n",
    "\n",
    "# # Define your RNN cell with two neural networks\n",
    "# class CustomRNNCell(tf.keras.layers.Layer):\n",
    "#     def __init__(self, output_size):\n",
    "#         super(CustomRNNCell, self).__init__()\n",
    "#         self.rnn = tf.keras.layers.SimpleRNN(output_size, return_sequences=True)\n",
    "#         self.fc = tf.keras.layers.Dense(2 * n, activation='sigmoid')\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         out = self.rnn(x)\n",
    "#         out_l, out_u = tf.split(self.fc(out), num_or_size_splits=2, axis=-1)\n",
    "#         return out_l, out_u\n",
    "\n",
    "\n",
    "\n",
    "# n_samples = 1\n",
    "# input_data = []\n",
    "# return_samples= []\n",
    "# cnt0 = 0\n",
    "# plot_x = []\n",
    "# for i in range(n_samples):\n",
    "#     sample = generate_monte_carlo_sample()\n",
    "#     if(sample[0][0]==0):\n",
    "#         cnt0+=1\n",
    "#     returns_for_this_sample = []\n",
    "#     for time in range(T):\n",
    "#         returns_for_this_sample.append(sample[time][1])\n",
    "#     return_samples.append(returns_for_this_sample)\n",
    "#     #return_samples.append()\n",
    "#     initial_p = np.random.rand(N)\n",
    "#     plot_x.append(initial_p[0])\n",
    "#     initial_p/=np.sum(initial_p)\n",
    "#     input_to_rnn = [initial_p]\n",
    "#     for time in range(T-1):\n",
    "#         p = input_to_rnn[-1]\n",
    "#         p_new = updateBelief_for_RNN(sample[time][1],p)\n",
    "#         #p_new = closest_probability_distn(p_new)[1]\n",
    "#         plot_x.append(p_new[0])\n",
    "#         input_to_rnn.append(p_new)\n",
    "#     input_data.append(input_to_rnn)\n",
    "\n",
    "# print(cnt0)\n",
    "# print(len(plot_x))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(plot_x,bins = 100)\n",
    "# plt.show()\n",
    "\n",
    "# input_data = tf.constant(input_data, dtype=tf.float32)\n",
    "# return_samples = tf.constant(return_samples, dtype=tf.float32)\n",
    "\n",
    "# # Create the model\n",
    "# rnn_cell = CustomRNNCell(output_size=2 * n)\n",
    "# custom_loss = CustomLossTF(crra_coefficient=1)\n",
    "\n",
    "# # Create an optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# # Training loop\n",
    "# for iter in range(10):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         lb, ub = rnn_cell(input_data)\n",
    "#         print(lb,ub)\n",
    "#         initial_wealth = tf.constant(1000.0,dtype = tf.float32)\n",
    "#         initial_portfolio = tf.constant(np.random.rand(n+1),dtype = tf.float32)\n",
    "#         initial_portfolio /= tf.reduce_sum(initial_portfolio)\n",
    "#         loss = custom_loss(\n",
    "#             initial_w=initial_wealth,\n",
    "#             initial_p=initial_portfolio,\n",
    "#             upper_b=ub,\n",
    "#             lower_b=lb,\n",
    "#             returns=return_samples,\n",
    "#             input_data=input_data\n",
    "#         )\n",
    "#     print(\"Loss: \", loss)\n",
    "#     grads = tape.gradient(loss, rnn_cell.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(grads, rnn_cell.trainable_variables))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define your constants\n",
    "transaction_rate = 0.005\n",
    "crra_coefficient = -1  # CRRA coefficient, adjust as needed\n",
    "\n",
    "def newWealthWithTC(W, r, pi_pre, pi_new):\n",
    "    W_new = tf.reduce_sum(W * pi_pre * (1 + r))\n",
    "    W_new = W_new - W_new * transaction_rate * tf.norm(pi_new - pi_pre, ord=1)\n",
    "    #print(W,r,pi_pre,pi_new,W_new)\n",
    "    return W_new\n",
    "\n",
    "# Define the custom loss function\n",
    "def custom_loss(initial_wealth, initial_portfolio, upper_b, lower_b, returns):\n",
    "    input_size = tf.shape(upper_b)[0]\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for _ in range(input_size):\n",
    "        wealth = [tf.constant([initial_wealth], dtype=tf.float32)]\n",
    "        portfolio = [tf.constant(initial_portfolio, dtype=tf.float32)]\n",
    "\n",
    "        #constraint_loss = 0.0\n",
    "        for i in range(T):\n",
    "            w = wealth[-1]\n",
    "            pi = portfolio[-1]\n",
    "            pi_new = tf.identity(pi)[:-1]\n",
    "\n",
    "            # constraint_loss += tf.reduce_sum(tf.nn.relu(l_b - u_b)) * 1000.0\n",
    "\n",
    "            # pi_new = tf.maximum(l_b, tf.minimum(u_b, pi_new))\n",
    "            # risk_free_allocation = 1 - tf.reduce_sum(pi_new)\n",
    "\n",
    "            closest_p_ind, __ = closest_probability_distn_TF(input_data[_, i])\n",
    "            pi_star = tf.convert_to_tensor(optimal_pi_star[(closest_p_ind, i)],dtype = tf.float32)\n",
    "            #pi_star_arrays.append(pi_star)\n",
    "            #print(pi_star)\n",
    "            pi_star = pi_star[:-1]\n",
    "            u_b = upper_b[_, i]\n",
    "            l_b = lower_b[_, i]\n",
    "            u_b = tf.minimum(tf.ones(n, dtype=tf.float32), pi_star + u_b)\n",
    "            l_b = tf.maximum(-tf.ones(n, dtype=tf.float32), pi_star - l_b)\n",
    "            #print(l_b,u_b)\n",
    "            # Your logic for updating u_b, l_b, and pi_new goes here\n",
    "            pi_new = tf.maximum(l_b, tf.minimum(u_b, pi_new))\n",
    "            risk_free_allocation = 1 - tf.reduce_sum(pi_new)\n",
    "            pi_new = tf.concat([pi_new, [risk_free_allocation]], axis=0)\n",
    "            #pi_new = tf.concat([pi_new, [risk_free_allocation]], axis=0)\n",
    "            portfolio.append(pi_new)\n",
    "\n",
    "            new_wealth = newWealthWithTC(w, returns[_][i], pi, pi_new)\n",
    "            wealth.append(new_wealth)\n",
    "        # print(wealth)\n",
    "        # print(portfolio)\n",
    "        loss = -((wealth[-1] ** crra_coefficient) / crra_coefficient) \n",
    "        #print(wealth)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss / tf.cast(input_size, dtype=tf.float32)\n",
    "\n",
    "# Define the RNN cell with two neural networks\n",
    "class CustomRNNCell(tf.keras.layers.Layer):\n",
    "    # def __init__(self, input_size, output_size):\n",
    "    #     super(CustomRNNCell, self).__init__()\n",
    "    #     self.input_size = input_size\n",
    "    #     self.output_size = output_size\n",
    "\n",
    "    #     self.fc1 = tf.keras.Sequential([\n",
    "    #         tf.keras.layers.Dense(80, activation='tan h'),\n",
    "    #         tf.keras.layers.Dense(40, activation='tanh'),\n",
    "    #         tf.keras.layers.Dense(20, activation='tanh'),\n",
    "    #         tf.keras.layers.Dense(output_size, activation='tanh')\n",
    "    #     ])\n",
    "\n",
    "    # def call(self, x):\n",
    "    #     out = self.fc1(x)\n",
    "    #     lower_b = out\n",
    "    #     upper_b = out\n",
    "    #     return lower_b, upper_b\n",
    "    def __init__(self, output_size):\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "        #self.rnn = tf.keras.layers.SimpleRNN(output_size, return_sequences=True)\n",
    "        self.fc1 = tf.keras.layers.Dense(n, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.fc2 = tf.keras.layers.Dense(n, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "    def call(self, x):\n",
    "        # out = self.rnn(x)\n",
    "        # out_l, out_u = tf.split(self.fc(out), num_or_size_splits=2, axis=-1)\n",
    "        out_l = self.fc1(x)\n",
    "        out_u = self.fc2(x)\n",
    "        return out_l, out_u\n",
    "\n",
    "# # Generate some example data\n",
    "# n_samples = 1\n",
    "# input_data = []\n",
    "# return_samples = []\n",
    "\n",
    "# for i in range(n_samples):\n",
    "#     sample = generate_monte_carlo_sample()  # You should implement this function\n",
    "#     returns_for_this_sample = [sample[time][1] for time in range(T)]\n",
    "#     return_samples.append(returns_for_this_sample)\n",
    "\n",
    "#     initial_p = np.random.rand(N)\n",
    "#     initial_p /= np.sum(initial_p)\n",
    "#     input_to_rnn = [initial_p]\n",
    "\n",
    "#     for time in range(T - 1):\n",
    "#         p = input_to_rnn[-1]\n",
    "#         p_new = updateBelief_for_RNN(sample[time][1], p)  # You should implement this function\n",
    "#         input_to_rnn.append(p_new)\n",
    "\n",
    "#     input_data.append(input_to_rnn)\n",
    "\n",
    "# input_data = tf.constant(input_data, dtype=tf.float32)\n",
    "# return_samples = tf.constant(return_samples, dtype=tf.float32)\n",
    "n_samples = 200\n",
    "input_data = []\n",
    "return_samples= []\n",
    "cnt0 = 0\n",
    "plot_x = []\n",
    "for i in range(n_samples):\n",
    "    sample = generate_monte_carlo_sample()\n",
    "    if(sample[0][0]==0):\n",
    "        cnt0+=1\n",
    "    returns_for_this_sample = []\n",
    "    for time in range(T):\n",
    "        returns_for_this_sample.append(sample[time][1])\n",
    "    return_samples.append(returns_for_this_sample)\n",
    "    #return_samples.append()\n",
    "    initial_p = np.random.rand(N)\n",
    "    initial_p/=np.sum(initial_p)\n",
    "    initial_p = closest_probability_distn(initial_p)[1]\n",
    "    plot_x.append(initial_p[0])\n",
    "    input_to_rnn = [initial_p]\n",
    "    for time in range(T-1):\n",
    "        p = input_to_rnn[-1]\n",
    "        p_new,__ = updateBelief(sample[time][1],p)\n",
    "        p_new = closest_probability_distn(p_new)[1]\n",
    "        plot_x.append(p_new[0])\n",
    "        input_to_rnn.append(p_new)\n",
    "    input_data.append(input_to_rnn)\n",
    "\n",
    "print(cnt0)\n",
    "print(len(plot_x))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(plot_x,bins = 100)\n",
    "plt.show()\n",
    "\n",
    "input_data = tf.constant(input_data, dtype=tf.float32)\n",
    "return_samples = tf.constant(return_samples, dtype=tf.float32)\n",
    "# Create the RNN cell and optimizer\n",
    "input_size = N\n",
    "output_size = n\n",
    "rnn_cell_57 = CustomRNNCell(output_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "#Training loop\n",
    "for iter in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        lb, ub = rnn_cell_57(input_data)\n",
    "        initial_wealth = 1000\n",
    "        initial_portfolio = np.random.rand(n + 1).astype(np.float32)\n",
    "        initial_portfolio /= np.sum(initial_portfolio)\n",
    "        loss = custom_loss(initial_wealth, initial_portfolio, ub, lb, return_samples)\n",
    "\n",
    "    gradients = tape.gradient(loss, rnn_cell_57.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, rnn_cell_57.trainable_variables))\n",
    "\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_values = []\n",
    "upper_b = []\n",
    "lower_b = []\n",
    "portfolio_values = []\n",
    "for p in possible_probabities:\n",
    "    p_normal = p[0]\n",
    "    x_values.append(p_normal)\n",
    "    pi_star = optimal_pi_star[(reverse_index[tuple(p)],1)]\n",
    "    portfolio_values.append(pi_star[0])\n",
    "    l_b,u_b = rnn_cell_57(tf.constant([[p]*T],dtype = tf.float32))\n",
    "    print(p,l_b[0][1][0],u_b[0][1][0])\n",
    "    lower_b.append(max(-1.0,(pi_star[0]-l_b[0][1][0])))\n",
    "    upper_b.append(min(1.0,(pi_star[0]+u_b[0][1][0])))\n",
    "plt.plot(x_values,lower_b,color= 'g')\n",
    "plt.plot(x_values,upper_b,color = 'b')\n",
    "plt.plot(x_values,portfolio_values,color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_input= tf.constant(temp_input,dtype = tf.float32)\n",
    "for p in possible_probabities:\n",
    "    print(p)\n",
    "    temp_input = np.zeros((1,T,N))\n",
    "    for j in range(T):\n",
    "        temp_input[0][j] = tf.constant(p,dtype = tf.float32)\n",
    "    temp_input= tf.constant(temp_input,dtype = tf.float32)\n",
    "    lb, ub = rnn_cell(temp_input)\n",
    "    lb = lb[0][0][0]\n",
    "    ub = ub[0][0][0]\n",
    "    print(lb,ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xpoints = [x[0] for x in possible_probabities]\n",
    "ypoints = [optimal_pi_star[_,2][0] for _ in range(len(possible_probabities))]\n",
    "\n",
    "plt.plot(xpoints,ypoints)\n",
    "plt.show()\n",
    "\n",
    "print([optimal_pi_star[_,2] for _ in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb,ub = rnn_cell(input_data)\n",
    "print(lb[0])\n",
    "print(ub[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "transaction_rate = 0.1\n",
    "\n",
    "def newWealth_withTC(W, r,pi_pre, pi_new):\n",
    "    # print(pi_new)\n",
    "    # print(pi_pre)\n",
    "    W = np.multiply(pi_pre,1+r)\n",
    "    diff = pi_pre-pi_new\n",
    "    W_new = W - transaction_rate * tf.norm(diff, ord=1)\n",
    "    return W_new\n",
    "\n",
    "class CustomLoss(keras.losses.Loss):\n",
    "    def __init__(self, crra_coefficient, **kwargs):\n",
    "        super(CustomLoss, self).__init__(**kwargs)\n",
    "        self.crra_coefficient = crra_coefficient\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        initial_wealth, initial_portfolio = y_true\n",
    "        lower_b, upper_b = y_pred\n",
    "        wealth = [tf.convert_to_tensor(initial_wealth)]\n",
    "        portfolio = [tf.convert_to_tensor(initial_portfolio)]\n",
    "\n",
    "        for i in range(T):\n",
    "            w = wealth[-1]\n",
    "            pi = portfolio[-1]\n",
    "            pi_new = tf.identity(pi)\n",
    "            pi_new = pi_new[:-1]\n",
    "            u_b = upper_b[0][i]\n",
    "            l_b = lower_b[0][i]\n",
    "            print(u_b)\n",
    "            print(l_b)\n",
    "            print(pi_new)\n",
    "            pi_new = tf.maximum(l_b,tf.minimum(u_b,pi_new))\n",
    "\n",
    "\n",
    "            risk_free_allocation = tf.constant(1.0) - tf.reduce_sum(pi_new)\n",
    "            pi_new = tf.concat([pi_new, [risk_free_allocation]], axis=0)\n",
    "            portfolio.append(pi_new)\n",
    "            new_wealth = newWealth_withTC(w, pi, pi_new)\n",
    "            wealth.append(new_wealth)\n",
    "\n",
    "        final_wealth = wealth[-1]\n",
    "        loss = -tf.pow(final_wealth, gamma) / gamma\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Define your RNN cell as a custom Keras layer\n",
    "class CustomRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, output_size, **kwargs):\n",
    "        super(CustomRNNCell, self).__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.rnn = keras.layers.RNN(keras.layers.SimpleRNNCell(output_size), return_sequences=True, batch_size = 1)\n",
    "        self.lower_bound_dense = keras.layers.Dense(n)\n",
    "        self.upper_bound_dense = keras.layers.Dense(n)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.rnn(x)\n",
    "        lower_b = self.lower_bound_dense(out)\n",
    "        upper_b = self.upper_bound_dense(out)\n",
    "        return lower_b, upper_b\n",
    "\n",
    "# Example usage of the custom loss function and RNN cell\n",
    "N = 2\n",
    "n = 1\n",
    "T = 5\n",
    "input_size = N  # Replace with the appropriate input size\n",
    "output_size = 2 * n  # 2 output units, \"no-trade\" and \"x-zone\" decisions\n",
    "crra_coefficient = -1  # CRRA coefficient, adjust as needed\n",
    "\n",
    "\n",
    "rnn_cell = CustomRNNCell(output_size)\n",
    "custom_loss = CustomLoss(crra_coefficient)\n",
    "\n",
    "# Generate some example data\n",
    "input_data = np.random.randn(1,T, N)  # (sequence_length, input_size)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Forward pass and compute loss\n",
    "for iter in range(5):\n",
    "    lb, ub = rnn_cell(input_data)\n",
    "    print(lb, ub)\n",
    "    \n",
    "    initial_wealth = 1\n",
    "    initial_portfolio = np.ones(n+1,dtype=\"float32\") / (n + 1)\n",
    "    \n",
    "    loss = custom_loss([initial_wealth, initial_portfolio], [lb,ub])\n",
    "\n",
    "    print(loss)\n",
    "    \n",
    "    grads = custom_loss.gradient([initial_wealth, initial_portfolio, ub, lb], [0])\n",
    "    optimizer.apply_gradients(zip(grads, rnn_cell.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the RNN parameters\n",
    "input_size = 10  # Input feature size\n",
    "hidden_size = 20  # Hidden state size\n",
    "num_layers = 2  # Number of RNN layers\n",
    "sequence_length = 5  # Length of the input sequence\n",
    "batch_size = 3  # Number of sequences in a batch\n",
    "\n",
    "# Create sample input data\n",
    "input_data = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "# Define the RNN layer\n",
    "rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "# Pass the input data through the RNN\n",
    "output, hidden = rnn(input_data)\n",
    "\n",
    "# 'output' contains the output at each time step\n",
    "# 'hidden' contains the hidden state at the final time step\n",
    "\n",
    "# Print the shapes of output and hidden\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Hidden shape:\", hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameters, including the output size (2 * n)\n",
    "n = 5  # You can change the value of n according to your problem\n",
    "output_size = 2 * n\n",
    "\n",
    "# Define the custom RNN cell\n",
    "class CustomRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_size):\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "        self.rnn = tf.keras.layers.SimpleRNN(output_size, return_sequences=True)\n",
    "        self.fc = tf.keras.layers.Dense(2 * n, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.rnn(x)\n",
    "        out_l, out_u = tf.split(self.fc(out), num_or_size_splits=2, axis=-1)\n",
    "        return out_l, out_u\n",
    "\n",
    "# Create an instance of the custom RNN cell\n",
    "rnn_cell = CustomRNNCell(output_size)\n",
    "\n",
    "# Define some input sequences for testing\n",
    "input_seq1 = tf.constant([[[0.2, 0.8], [0.5, 0.5], [0.7, 0.3]]], dtype=tf.float32)  # Example input 1\n",
    "input_seq2 = tf.constant([[[0.6, 0.4], [0.4, 0.6], [0.1, 0.9]]], dtype=tf.float32)  # Example input 2\n",
    "input_seq3 = tf.constant([[[0.3, 0.7], [0.2, 0.8], [0.9, 0.1]]], dtype=tf.float32)  # Example input 3\n",
    "\n",
    "# Obtain initial outputs for the input sequences\n",
    "initial_outputs1 = rnn_cell(input_seq1)\n",
    "initial_outputs2 = rnn_cell(input_seq2)\n",
    "initial_outputs3 = rnn_cell(input_seq3)\n",
    "\n",
    "# Print the initial outputs\n",
    "print(\"Initial Outputs for Input Sequence 1:\")\n",
    "print(\"Lower Bounds:\")\n",
    "print(initial_outputs1[0].numpy())\n",
    "print(\"Upper Bounds:\")\n",
    "print(initial_outputs1[1].numpy())\n",
    "print()\n",
    "\n",
    "print(\"Initial Outputs for Input Sequence 2:\")\n",
    "print(\"Lower Bounds:\")\n",
    "print(initial_outputs2[0].numpy())\n",
    "print(\"Upper Bounds:\")\n",
    "print(initial_outputs2[1].numpy())\n",
    "print()\n",
    "\n",
    "print(\"Initial Outputs for Input Sequence 3:\")\n",
    "print(\"Lower Bounds:\")\n",
    "print(initial_outputs3[0].numpy())\n",
    "print(\"Upper Bounds:\")\n",
    "print(initial_outputs3[1].numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
